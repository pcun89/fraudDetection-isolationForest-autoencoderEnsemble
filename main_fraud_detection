from __future__ import annotations
import os
import argparse
import json
from typing import Tuple, Optional, Dict, Any

import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score,
    precision_recall_curve,
    auc,
    confusion_matrix,
    classification_report,
)
import joblib
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import torch.optim as optim
from tqdm import tqdm

# Constants
RANDOM_STATE = 42
ISO_MODEL_PATH = "iso_forest.joblib"
AE_MODEL_PATH = "autoencoder.pth"
SCALER_PATH = "scaler.joblib"

# -----------------------------
# Data utilities
# -----------------------------


def loadOrGenerateData(csvPath: Optional[str] = None, labelCol: str = "label", testSize: float = 0.3
                       ) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:
    """
    Load CSV with numeric features and a label column, or generate synthetic imbalanced dataset.
    Returns: X_train_df, y_train, X_test_df, y_test
    """
    if csvPath:
        df = pd.read_csv(csvPath)
        if labelCol not in df.columns:
            raise ValueError(f"Label column '{labelCol}' not found in CSV.")
        y = df[labelCol].values
        X = df.drop(columns=[labelCol])
    else:
        # synthetic imbalanced classification with rare fraud class
        X, y = make_classification(
            n_samples=5000,
            n_features=20,
            n_informative=8,
            n_redundant=2,
            n_clusters_per_class=1,
            weights=[0.99, 0.01],  # 1% fraud
            flip_y=0.01,
            random_state=RANDOM_STATE,
        )
        X = pd.DataFrame(X, columns=[f"f{i}" for i in range(X.shape[1])])
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=testSize, random_state=RANDOM_STATE, stratify=y)
    return X_train, pd.Series(y_train), X_test, pd.Series(y_test)

# -----------------------------
# Autoencoder (PyTorch) definition
# -----------------------------


class SimpleAutoencoder(nn.Module):
    def __init__(self, nFeatures: int, hiddenDim: int = 64, latentDim: int = 8):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(nFeatures, hiddenDim),
            nn.ReLU(),
            nn.Linear(hiddenDim, latentDim),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(latentDim, hiddenDim),
            nn.ReLU(),
            nn.Linear(hiddenDim, nFeatures),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encoder(x)
        recon = self.decoder(z)
        return recon


def trainAutoencoder(model: nn.Module, dataLoader: DataLoader, nEpochs: int = 10, lr: float = 1e-3, device: str = "cpu"):
    model.to(device)
    opt = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    for epoch in range(1, nEpochs + 1):
        model.train()
        running = 0.0
        count = 0
        for xb in dataLoader:
            xb = xb[0].to(device)
            opt.zero_grad()
            recon = model(xb)
            loss = criterion(recon, xb)
            loss.backward()
            opt.step()
            running += loss.item() * xb.size(0)
            count += xb.size(0)
        avg = running / max(1, count)
        print(f"[AE] Epoch {epoch}/{nEpochs} loss={avg:.6f}")


def computeReconstructionErrors(model: nn.Module, X: np.ndarray, device: str = "cpu", batchSize: int = 256) -> np.ndarray:
    model.to(device)
    model.eval()
    ds = TensorDataset(torch.from_numpy(X.astype(np.float32)))
    loader = DataLoader(ds, batch_size=batchSize, shuffle=False)
    errors = []
    with torch.no_grad():
        for (xb,) in loader:
            xb = xb.to(device)
            recon = model(xb)
            err = torch.mean((recon - xb) ** 2, dim=1).cpu().numpy()
            errors.extend(err.tolist())
    return np.array(errors)

# -----------------------------
# Helpers: normalize scores to [0,1]
# -----------------------------


def minMaxNormalize(arr: np.ndarray) -> np.ndarray:
    a = float(np.min(arr))
    b = float(np.max(arr))
    if b - a < 1e-12:
        return np.zeros_like(arr)
    return (arr - a) / (b - a)


# -----------------------------
# Pipeline: fit detectors + score ensemble
# -----------------------------
def fitIsolationForest(X_train_scaled: np.ndarray, nEstimators: int = 100, contamination: float = 0.01) -> IsolationForest:
    iso = IsolationForest(n_estimators=nEstimators,
                          contamination=contamination, random_state=RANDOM_STATE)
    iso.fit(X_train_scaled)
    joblib.dump(iso, ISO_MODEL_PATH)
    print("Saved IsolationForest ->", ISO_MODEL_PATH)
    return iso
